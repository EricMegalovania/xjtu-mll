# 上午听课

继续概率论

## Jensen Inequality

对于凸函数，
$$
E[f(x)]\ge f[E(x)]
$$
其中 x 是概率分布 D 上的随机变量

## Hoeffding's Inequality

## 统计

## PAC

Probably Approximately Correct

correct$\ge 1-\delta$

error$\le\epsilon$ is correct

## 线性回归

记住两点

Evaluation：OLS

Optimisation：直接解方程或者（随机）梯度下降

## 多项式回归

# 作业

## 线性回归

$$
w_{OLS}=(X^TX)^{-1}X^Ty
$$

这个就是，给定 training data 矩阵 X 和 target data 向量 y，我们要寻找最好的 w 使得
$$
y=X'\cdot w_{best}
$$
其中 X' 的第一列为 1，对应一次函数里 $kx+b$ 的 $b$

```python
def simple_linear_regression(X, y):
    '''
    y = XX @ w ????
    w = XX.inv() @ y
    '''
    XX = np.hstack((np.ones(X.shape[0]).reshape(X.shape[0], 1), X))
    XXT = XX.T
    w_best = LA.inv(XXT @ XX) @ XXT @ y
    return w_best

def pred(w, X_test):
    XX = np.hstack((np.ones(X_test.shape[0]).reshape(X_test.shape[0], 1), X_test))
    y_pred = XX @ w
    return y_pred
```

## 多个特征的线性回归

这个拿 scikit-learn 库自带的 boston 房价数据集来"bench mark"，没有需要新写代码的部分

## 多项式用线性回归

仍然是"bench mark"性质为主，主要学习 `PolyFeatures` 相关语法

```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

# 假设你有一些原始特征数据 X
X = np.array([[1], [2], [3], [4], [5]])

# 创建 PolynomialFeatures 实例，设置多项式的度数为2
poly = PolynomialFeatures(degree=2)

# 生成多项式特征
X_poly = poly.fit_transform(X)

print(X_poly)
```

```text
[[ 1.  1.  1.]
 [ 1.  2.  4.]
 [ 1.  3.  9.]
 [ 1.  4. 16.]
 [ 1.  5. 25.]]
```

这个例子主要告诉我们，拟合的多项式次数越高，拟合的越准确，甚至有可能把噪声也拟合进去

提升多项式次数，本质是偏差/方差间的取舍，并不是次数越高越好

## 正则化

Regularisation

为解决拟合采用的多项式次数过高时产生的过拟合问题

$l_2/l_1$ 正则化

$l_2$：Ridge Regression 岭回归，把系数的平方加进了 loss function

$l_1$：LASSO Regression，把系数的绝对值加进了 loss function

## 梯度下降

```python
def gradient_linear_regression(X, y, n_iterations):
    eta = 0.01
    n_iterations = n_iterations
    m = X.shape[0]
    w_best = np.random.randn(X.shape[1] + 1, 1)
    X=np.hstack((np.ones((m,1)),X))
    for i in range(n_iterations):
        y_new = X @ w_best
        grad = (1/m) * X.T @ (y_new - y)  # 1/m, 2/m 都行
        w_best = w_best - eta * grad
    return w_best
```
