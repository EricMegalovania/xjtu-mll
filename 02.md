# 上午听课

## 线代基础

一般机器学习都在**欧式空间**

一般以列向量作为一个样本

然后各种基础知识

### 子空间

$U,\ W$ 是两个线性空间
$$
U+W=\{u+w|u\in U,\ w\in W\}
$$

$$
\dim(U+W)=\dim(U)+\dim(W)-\dim(U\cap W)
$$

如果 $U\cap W=\emptyset$，则记 $U+W=U\oplus W$

Lines, Planes, Hyperplanes

## 矩阵基础

不同角度看矩阵：
$$
a,b\in\R^{n\times1},\ M\in\R^{n\times n}\\
a=Mb
$$
可以把矩阵看成一个函数

如果把列向量看做变量，那么行向量就是函数

再比如：
$$
\dfrac{\partial u}{\partial x}=2x
$$
对于这个偏导，我们不可能给机器学习丢进去无限个点，我们只能离散，丢进去一个二维矩阵，这个也是矩阵

### 线性变换 Linear Map

Matrices are linear maps
$$
[Ax]_i=\sum_{j=1}^{m}A_{ij}x_j\\
A_x=\sum_{j=1}^{m}A_jx_j
$$
其中 $A_j$ 是 $A$ 的第 $j$ 列构成的列向量，所以可以看成**函数的相加**
$$
C=AB
$$
则 $Cx$ 是更加复杂的函数的线性变换

### 其他

矩阵的转置，单位矩阵，矩阵的逆，对称矩阵

**nullspace** 零空间，指 $\forall v,\ Av=0$ 的 $A$

**range** 值域？或者指经过矩阵映射之后的空间

**行空间，列空间** columnspace(A)=range(A), rowspace(A)=range(A^T)

（就是以列或者以行作为向量的空间）

矩阵的 rank（zhi打不出来）

### 范数

就是数值分析里那一套

1-范数，2-范数，$\infty$-范数

### 内积

$$
\langle u,v\rangle
$$

定义它是为了比较向量/矩阵的相似程度

测度，相似度？

### 正交！！

$$
\langle u,v\rangle=0
$$

> 把大学比作机器学习？？？
>
> unsupervised training
>
> value怎么定义，没有测度
>
> ~~于是按照训练者喜好来决定value~~

回到课堂，标准正交基的概念

### 正交矩阵

$$
QQ^T=Q^TQ=I\\
Q^T=Q^{-1}
$$

e.g. 旋转

### Projection 射影

射影，求射影就是先内积，然后除以向量的模长得到射影的长度，再除以一遍以确定方向

矩阵也有射影，几何意义看作是面的投射

normal equations???

## 线性方程

$$
Ax=b
$$

唯一解，无解，多解？

寻找解空间：找线性无关的解作为基

- rank(A) < rank(A|b)：有矛盾的方程式，无解
- rank(A) = rank(A|b) < m：多解
- rank(A) = rank(A|b) = m <=n：唯一解

如何解？

- 高斯消元，$O(n^3)$
- 下文会提

$$
x=A^{-1}b
$$

出现逆需要警觉，矩阵不一定有逆

### 矩阵对应的行列式

**determinant**
$$
\det(A)
$$
一堆行列式基础性质，最后一条性质的几何解释，放大之后**面积/体积**的关系
$$
\det(\alpha A)=\alpha^n\det(A)
$$

### 矩阵的对角线之和

tr(A)

### 可逆矩阵

- 满秩矩阵
- etc.

### 特征值

$$
\det(A-\lambda I)=0
$$

解 $\lambda$ 即可

> [!Note]
>
> 这种方程比较好解？选模型会考虑这一点？

对称矩阵的特征值，后面有一些没来得及记

### 二次型

$$
\because x^TMx=x^TM^Tx\\
\therefore x^TMx=x^TAx
$$

其中 $A=\dfrac{1}{2}(M+M^T)$，是对称矩阵，可以运用上方特征值的良好性质

所以我们之后用 $A$ 来代替 $M$

二次型与优化的关系

$xA^Tx$ 不变号，则是正定/负定

其余是不定

几何性质：起码不会反向

划分？然后变成类似二次函数的形式

## Useful Results

- $X^TX$ 对称

- $x^TX^TXx\ge0$

- 若 $x^TX^TXx>0$，
- $rank(X^TX)=rank(X)$

TMD 等到PPT发了再说吧吧吧吧吧

## 微积分

导数的严格定义

泰勒展开

偏导

梯度，偏导的向量

方向导数

全微分，遵循链式法则

Jacobian 矩阵，多维2多维的导数

Hession 矩阵，多维2多维，从Jacobian的一阶偏导变成了二阶偏导

矩阵2一个数，也可以定义偏导

**矩阵微积分？？？**

$a^Tx$ 的“偏导”是 $a$

$x^TAx$ 的“偏导”是 $(A+A^T)x$

然后出现了奇奇怪怪的二次型的导数 :sweat:

主对角线 tr() 的“偏导”？？？:question:

多元函数的泰勒展开，高数和数值分析都讲过

**极值**

寻找范围：边界，梯度为 0 的点

鞍点：梯度为 0 但不是局部极值点

判断鞍点，从 Hession 矩阵是否正定/负定来判断

**凸包**

突然来到了计算几何，不过这个只是几何解释

对于函数，凸性
$$
f(\lambda x+(1-\lambda)y)\le\lambda f(x)+(1-\lambda)f(y)
$$
Concave 凹

就是把 $\le$ 换成 $\ge$

---

# 作业

## python的向量

```python
import numpy as np
video = np.array([10.5, 5.2, 3.25, 7.0])
video.size   # 向量长度
video[2]     # 3rd元素

import matplotlib.pyplot as plt
u = np.array([2, 5])   # 点u
v = np.array([3, 1])   # 点v
x_coords, y_coords = zip(u, v)  # x=[2,3] y=[5,1] 分别存了x坐标和y坐标
plt.scatter(x_coords, y_coords, color=["r", "b"]) # 绘制散点图, 需要x.size==y.size==color.size
plt.axis([0, 9, 0, 6]) # x轴[0,9] y轴[0,6]
plt.grid()  # 显示网格
plt.show()  # 显示图像
```

其他的画图直接见作业吧，太多了这里不粘贴了

```python
import numpy.linalg as LA

LA.norm(u)    # 计算模长, u就是上面的那个点
np.dot(u, v)  # 计算点积
u.dot(v)      # method of `ndarrays`
u @ v         # 计算点积, 需要 Python 3.5及以上
u * v         # [2,5]*[3,1]=[6,5] 会把每个元素相乘得到新的array

def vector_angle(u, v):  # 计算向量夹角, 返回 radian(s)
    cos_theta = u.dot(v) / LA.norm(u) / LA.norm(v)
    return np.arccos(cos_theta.clip(-1, 1))
# 为什么用clip
# 浮点数精度误差, 可能使cos_theta略大于1, 引起arccos错误
```

$v$ 在 $u$ 上的射影，
$$
\mathbf{proj_u v=(v\cdot\hat{u})\cdot\hat{u}}
$$
等式右边，左边是长度，右边是方向，$\hat{u}$ 表示 $u$ 这个方向的单位向量

```python
u_normalized = u / LA.norm(u)
proj = v.dot(u_normalized) * u_normalized
```

## python的矩阵

```python
A = np.array([
    [10, 20, 30],
    [40, 50, 60]
])

# 矩阵的属性
A.shape   # (2, 3)
A.size    # 6
A[1, 2]   # 2nd row, 3rd col, 60
A[1, :]   # array([40, 50, 60])
A[:, 2]   # array([30, 60])
A[1:2, :] # 取[1,2)行 array([[40, 50, 60]]) 1*3矩阵
A[:, 2:3] # 取[2,3)列 array([ [30], [60] ]) 2*1矩阵

# 对角矩阵
np.diag([4, 5, 6])
'''
array([[4, 0, 0],
       [0, 5, 0],
       [0, 0, 6]])
'''
D = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9],
])
np.diag(D)   # 传入矩阵, 则只提取出对角线
# array([1, 5, 9])

# 单位矩阵
np.eye(3)

# 下面两个都是矩阵乘法, @需要Python 3.5版本
C = np.matmul(A, D)
C = A @ D

A * B  # 不是矩阵乘法, 是对应位置元素相乘

# 转置
A.T
```

## 1D arrays 变成 2D arrays

```python
u                 # array([2, 5])
u.T               # array([2, 5])

# 变成1*2矩阵
np.array([u])     # array([[2, 5]])
u[np.newaxis, :]  # array([[2, 5]])
u[np.newaxis]     # 这俩可以理解成, 在2前面新加一个维度, shape变成(1, 2)
# 比如 u[np.newaxis][np.newaxis]即array([[[2, 5]]]), shape=(1, 1, 2)
u[None]           # 最短, 但是不建议因为不清晰, P.S. np.newaxis==None

# 变成2*1矩阵, 列向量
np.array([u]).T   # array([[2], [5]])
u[:, np.newaxis]  # array([[2], [5]])
```

## 矩阵的其他运算

```python
A = np.array([
    [1, 1, 4],
    [4, 1, 4],
    [1, 9, 1],
    [9, 8, 1]
])
A + [[1], [2], [3], [4]]
'''
array([
	[2,  2,  5],
	[6,  3,  6],
	[4,  12, 4],
	[13, 12, 5],
])
可以看成是每一个列向量都加上了(1,2,3,4)这个向量
'''

# 矩阵求逆
try:
    LA.inv(D)
except LA.LinAlgError as e:
    print("LinAlgError:", e)

# 矩阵的行列式
LA.det(D)

# SVD分解
U, s, Vt = LA.svd(D)  # U @ s @ Vt == D

# 特征值, 特征向量
eigenvalues, eigenvectors = LA.eig(D)

# 主对角线之和
D.trace()
```

